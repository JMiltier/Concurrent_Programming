# C++ Parallel Algorithms


#### Concurrent Tree
A tree, whether a binary search tree (BST), radix tree, red-black tree (RBT), or any other tree imagineable, allowing for multiple reads/writes/deletes functionalities for parallism. Ideally, all reads are lock-free, meaning any read threads should not block. These read threads should always see a consistent version of the tree, and should never block writing threads. As for the writing threads, they should block other writing threads, but not any reading threads. Writing threads blocking writing threads ensures consistency when modifying the tree. [Additional info](https://www.cs.cmu.edu/~yihans/papers/tutorial.pdf)

#### Concurrent Containers

#### Transactions
A series of code execution that only commits all if the entire run is completed successfully. Othwerise, it will abort (fail) and roll back. Doing this in parallel seems odd, being that it may be need to wait on one (sequentially) function to run before another. This seems extremely useful for database management. More importantly, there seems that there may be some additional troubleshooting for transactions that may not be discovered until further implemented. [Additional read](https://ashutoshtripathi.com/2017/11/28/concurrent-executions-in-transaction/)

#### Decision of Parallel Algorithm to implement
For the sake of resources available, and class slides, concurrent containers seemed to have the greatest amount of references. As discovered, these are added to the resources section at the end of this document. 

## Code Organization
#### üìÅ Files
  1. `Makefile` - used to create and remove executable C++11 objects 
  2. `container.cpp` - bucket sort using bars and locks
  3. `arg_parser.h` - parsing and error handling for mysort program input options
  6. `README.pdf` - write-up for project (this file)

#### üíæ Additional files
  1. `randomNumberGen.cpp`- creates a text file with a specified number of random numbers from [ 1 - number size ]. Default number is 10,000 unless specified when ran (as second argument).
  2. `pthread_add.h` - threading if compiling on macOS (will still run as normal for linux systems)'
  3. `perf.sh` - shell script file to run perf tests -- Omitted this file b/c was not able to run due to permissions on jupiter notebook. Methods for testing are listed below.

### Compiling
  Note: If zipped, first unzip file before proceeding.
  1. From root directory, run `make` from the terminal. This will generate a program called *counter* and *mysort*
  2. Next, follow execution syntax below for either *counter* or *mysort*.
  3. `Make clean` will remove all files generated by `make`.
  4. Additionally, `make numGen` will generate the executable object to generate random numbers to a .txt file (set to *source.txt*).

## Execution
#### Program option/input parameters for `counter`
```shell
> counter [--name]
```
`counter --name`: prints name to console.  
**OR**  
```shell
> counter [-t NUM THREADS] [-i=NUM_ITERATIONS] [--bar=<sense,pthread>] [--lock=<tas,ttas,ticket,mcs,pthread>] [-o out.txt]
```
  1. `-t NUM_THREADS`: specify how many threads to use during execution (including master thread)
  2. `-i=NUM_ITERATIONS`: number of times to increment counter on each thread
  3. `--bar=<sense,pthread>`: type of barrier algorithm to use
  4. `--lock=<tas,ttas,ticket,mcs,pthread>`: type of locking algorithm to use
  5. `-o out.txt`: sorted txt file of numebrs, in which the program outputs/writes to
  - **Additional outputs**: time of execution in nanoseconds  
Example of program execution, using 10 threads, 1000 iterations for each thread, TAS locking algorithm, and writing to file out.txt:
```shell
> ./counter -t 10 -i=1000 --lock=tas -o out.txt
```

## Analyzing program performances, using `perf`
## Barriers
#### For L1 cache hit rate
```shell
> perf stat -e L1-dcache-loads -e L1-dcache-load-misses ./counter -t 10 -i=1000 --bar=<sense, pthread> -o out.txt
> perf stat -e L1-dcache-loads -e L1-dcache-load-misses ./mysort source.txt -o out.txt -t 10 --alg=bucket --bar=<sense, pthread>
```
#### For branch-prediction hit rate
```shell
> perf stat -e branch-loads -e branch-load-misses ./counter -t 10 -i=1000 --bar=<sense, pthread> -o out.txt
> perf stat -e branch-loads -e branch-load-misses ./mysort source.txt -o out.txt -t 10 --alg=bucket --bar=<sense, pthread>
```
#### For L1 cache hit rate
```shell
> perf stat -e page-faults ./counter -t 10 -i=1000 --bar=<sense, pthread> -o out.txt
> perf stat -e page-faults ./mysort source.txt -o out.txt -t 10 --alg=bucket --bar=<sense, pthread>
```
#### Or all stats at once (how I ended up doing it), using 10 repeated runs
```shell
> perf stat --repeat 10 -e L1-dcache-loads,L1-dcache-load-misses,branch-loads,branch-load-misses,page-faults ./counter -t 10 -i=1000 --bar=<sense, pthread> -o out.txt  
> perf stat --repeat 10 -e L1-dcache-loads,L1-dcache-load-misses,branch-loads,branch-load-misses,page-faults ./mysort source.txt -o out.txt -t 10 --alg=bucket --bar=<sense, pthread>
```

### Barrier Perf Table (Iteration/Array at 1000, 10 threads, and 10 repeated runs (average))
Program | Barrier | Run Time (s) | L1 cache hit rate | branch-prediction hit rate | page-fault
:------ | :-----: | :----------- | :---------------- | :------------------------- | :---------
counter | sense   | 0.001862     | 99.70%            | 99.86%                     | 154
counter | pthread | 0.012391     | 90.04%            | 99.01%                     | 148
mysort  | sense   | 0.001578     | 99.32%            | 99.46%                     | 187
mysort  | pthread | 0.000531     | 96.56%            | 97.58%                     | 186

## Locks
#### For L1 cache hit rate
```shell
> perf stat -e L1-dcache-loads -e L1-dcache-load-misses ./counter -t 10 -i=1000 --lock=<tas,ttas,ticket,pthread> -o out.txt
> perf stat -e L1-dcache-loads -e L1-dcache-load-misses ./mysort source.txt -o out.txt -t 10 --alg=bucket --lock=<tas,ttas,ticket,pthread>
```
#### For branch-prediction hit rate
```shell
> perf stat -e branch-loads -e branch-load-misses ./counter -t 10 -i=1000 --lock=<tas,ttas,ticket,pthread> -o out.txt
> perf stat -e branch-loads -e branch-load-misses ./mysort source.txt -o out.txt -t 10 --alg=bucket --lock=<tas,ttas,ticket,pthread>
```
#### For L1 cache hit rate
```shell
> perf stat -e page-faults ./counter -t 10 -i=1000 --lock=<tas,ttas,ticket,pthread> -o out.txt
> perf stat -e page-faults ./mysort source.txt -o out.txt -t 10 --alg=bucket --lock=<tas,ttas,ticket,pthread>
```
#### Or all stats at once (how I ended up doing it), using 10 repeated runs
```shell
> perf stat --repeat 10 -e L1-dcache-loads,L1-dcache-load-misses,branch-loads,branch-load-misses,page-faults ./counter -t 10 -i=1000 --lock=<tas,ttas,ticket,pthread> -o out.txt 
> perf stat --repeat 10 -e L1-dcache-loads,L1-dcache-load-misses,branch-loads,branch-load-misses,page-faults ./mysort source.txt -o out.txt -t 10 --alg=bucket --lock=<tas,ttas,ticket,pthread>
```

### Lock Perf Table (Iteration/Array at 1000, 10 threads, and 10 repeated runs (average))
Program | Lock    | Run Time (s) | L1 cache hit rate | branch-prediction hit rate | page-fault
:--------- | :------ | :----------- | :---------------- | :------------------------- | :---------
counter | tas     | 0.000574     | 92.37%            | 97.01%                     | 144
counter | ttas    | 0.000705     | 93.40%            | 97.07%                     | 145
counter | ticket  | 0.001021     | 96.17%            | 98.69%                     | 145
counter | pthread | 0.001050     | 97.06%            | 99.01%                     | 145
mysort  | tas     | 0.000346     | 96.99%            | 97.75%                     | 171
mysort  | ttas    | 0.000371     | 97.02%            | 97.78%                     | 170
mysort  | ticket  | 0.000358     | 97.30%            | 97.92%                     | 166
mysort  | pthread | 0.000349     | 96.53%            | 97.46%                     | 171

### Perf Analysis
Overall among benchmarking and mysort performance tests, locking algorithms seem to be faster and have better hit rates. Speculating that with barriers, each process has to wait for one another before proceeding on, while the locking algorithms are able to sort the subarray independently on one another prior to merging back. The pthread appeared the perform the worse across the board, since each thread was having to synchronize after every bit, there's a lot of overhead cost involed. The fastest looked to be TAS lock, imagining it's the simplest of the locks, but it not scalable so could see degraded results with larger numbers. 

#### üêú Surviving Bugs
  - Input/output files are loaded/saved as is. Does not check for txt file types or if data is integers.
  - Creating/Joining of threads is out of order for locks, so sometimes fails bucket sorting.

#### Resources:
1. [Scalable Lock-free Stack Algorithm](https://people.csail.mit.edu/shanir/publications/Lock_Free.pdf)
2. [Flat Combining the Synchronization-Parallelism Tradeoff](http://mcg.cs.tau.ac.il/papers/spaa2010-fc.pdf)
3. [The Baskets Queue](https://people.csail.mit.edu/shanir/publications/Baskets%20Queue.pdf) 
4. [Concurrent Queue](https://github.com/cameron314/concurrentqueue)

<!-- CREATE README PDF -->
<!-- md2pdf README.md --highlight-style atom-one-dark --pdf-options '{ "format": "Letter", "margin": "20mm", "printBackground": true }' -->